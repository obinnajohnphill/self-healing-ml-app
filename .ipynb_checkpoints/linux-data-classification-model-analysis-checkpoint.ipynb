{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ad01b4-eec2-4d12-be85-c5de10117fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Android:\n",
      "0    1400773\n",
      "1     154232\n",
      "Name: error, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SMote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m X_preprocessed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Apply SMOTE to handle class imbalance\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMote(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     66\u001b[0m X_smote, y_smote \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_preprocessed, y)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Split the preprocessed data into training and testing sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMote' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define datasets paths\n",
    "datasets = {\n",
    "    'Android': 'dataset/system-logs/multiple-system-log-dataset/preprocessed-data/Android_preprocessed.csv',\n",
    "    'Linux': 'dataset/system-logs/multiple-system-log-dataset/preprocessed-data/Linux_preprocessed.csv',\n",
    "    'Mac': 'dataset/system-logs/multiple-system-log-dataset/preprocessed-data/Mac_preprocessed.csv',\n",
    "    'Windows': 'dataset/system-logs/multiple-system-log-dataset/preprocessed-data/Windows_preprocessed.csv'\n",
    "}\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Support Vector Machine': SVC(probability=True),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Extra Trees': ExtraTreesClassifier()\n",
    "}\n",
    "\n",
    "for system_name, file_path in datasets.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(f\"Class distribution in {system_name}:\")\n",
    "    print(df['error'].value_counts())\n",
    "    \n",
    "    # Split data into features and target\n",
    "    X = df.drop('error', axis=1)\n",
    "    y = df['error']\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Define a preprocessor with OneHotEncoder for categorical columns and StandardScaler for numerical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Apply SMOTE to handle class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X_preprocessed, y)\n",
    "    \n",
    "    # Split the preprocessed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42, stratify=y_smote)\n",
    "    \n",
    "    # Compare models\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=10, scoring='roc_auc')\n",
    "        print(f\"{name} AUC: {scores.mean()} (+/- {scores.std()})\")\n",
    "        if scores.mean() > best_score:\n",
    "            best_score = scores.mean()\n",
    "            best_model = model\n",
    "    \n",
    "    # Tune the best model\n",
    "    param_grid = {\n",
    "        'Logistic Regression': {'C': [0.01, 0.1, 1, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100, 200]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100, 200]},\n",
    "        'Support Vector Machine': {'C': [0.1, 1, 10]},\n",
    "        'Decision Tree': {'max_depth': [None, 10, 20, 30]},\n",
    "        'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7, 9]},\n",
    "        'AdaBoost': {'n_estimators': [50, 100, 200]},\n",
    "        'Extra Trees': {'n_estimators': [50, 100, 200]},\n",
    "        'Multinomial Naive Bayes': {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(best_model, param_grid.get(type(best_model).__name__, {}), cv=10, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"Final tuned model performance for {system_name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC: {roc_auc_score(y_test, y_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999997b-26c0-4b87-9754-e95ad18c0df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
